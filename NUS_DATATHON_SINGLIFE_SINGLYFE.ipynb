{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
    "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas \n",
    "# %pip install matplotlib\n",
    "# %pip install graphviz\n",
    "# %pip install h2o\n",
    "# add commented pip installation lines for packages used as shown above for ease of testing\n",
    "# the line should be of the format %pip install PACKAGE_NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DO NOT CHANGE** the filepath variable\n",
    "##### Instead, create a folder named 'data' in your current working directory and \n",
    "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can have as many cells as you want for code\n",
    "filepath = \"./data/catB_train.parquet\" \n",
    "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALL** Code for machine learning and dataset analysis should be entered below. \n",
    "##### Ensure that your code is clear and readable.\n",
    "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cell below is **NOT** to be removed\n",
    "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list). \n",
    "##### It is recommended to test the function out prior to submission\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
    "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTING LIBRARIES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h2o\n",
    "from h2o.estimators import H2ORandomForestEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data\n",
    "df = pd.read_parquet(filepath)\n",
    "\n",
    "#Reading the data and having an overview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
    "    '''DO NOT REMOVE THIS FUNCTION.\n",
    "\n",
    "The function accepts a dataframe as input and return an iterable (list)\n",
    "of binary classes as output.\n",
    "\n",
    "The function should be coded to test on hidden data\n",
    "and should include any preprocessing functions needed for your model to perform. \n",
    "    \n",
    "All relevant code MUST be included in this function.'''\n",
    "    df = hidden_data\n",
    "\n",
    "    ## Cleaning\n",
    "\n",
    "    #1. Search for duplicates\n",
    "  \n",
    "    #2. Drop \"clntnum\" column\n",
    "    df.drop(\"clntnum\", axis=1, inplace=True)\n",
    "    \n",
    "    #3. Drop columns that have only one value\n",
    "    for col in df:\n",
    "        if df[col].nunique(dropna=False) == 1:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "             #57 columns dropped\n",
    "\n",
    "    df.isnull().sum().to_string()\n",
    "\n",
    "    #4. Drop the 1014 rows with NaN for the 20 key columns (not very informative samples)\n",
    "    df = df[~df[\"flg_substandard\"].isna()]\n",
    "    \n",
    "    #Studying the columns with null values\n",
    "    for column in df.columns:\n",
    "        if df[column].isnull().any():\n",
    "            p = df[column].isnull().sum()/16978*100\n",
    "            unique_values = df[column].unique()\n",
    "                        \n",
    "    #imputation by taking a random sample from non-null values in the column\n",
    "    def impute(df, column_to_impute):\n",
    "        non_null_values = df[column_to_impute].dropna().values\n",
    "        null_indices = df[df[column_to_impute].isnull()].index\n",
    "        df.loc[null_indices, column_to_impute] = np.random.choice(non_null_values, size=len(null_indices), replace=True)\n",
    "\n",
    "    #race_desc can use imputation by random sampling from non-null values.\n",
    "    impute(df, \"race_desc\")\n",
    "\n",
    "    #ctrycode_desc can change to \"Not Applicable\".\n",
    "    df['ctrycode_desc'].fillna(\"Not Applicable\", inplace=True)\n",
    "\n",
    "    #cltsex_fix can use imputation.\n",
    "    impute(df, \"cltsex_fix\")\n",
    "\n",
    "    #f_ever_declined_la can change to False (1 change to True)\n",
    "    df['f_ever_declined_la'].fillna(False, inplace=True)\n",
    "    df['f_ever_declined_la'] = df['f_ever_declined_la'].astype(bool)\n",
    "\n",
    "    #hh_size can be dropped (duplicate of hh_size_est)\n",
    "    a = df.shape[1]\n",
    "    df.drop(\"hh_size\", axis=1, inplace=True)\n",
    "\n",
    "    #hh_size_est can use imputation.\n",
    "    impute(df, \"hh_size_est\")\n",
    "\n",
    "    #annual_income_est can use imputation.\n",
    "    impute(df, \"annual_income_est\")\n",
    "\n",
    "    a = df.shape[1]\n",
    "    df.drop(\"hh_20\", axis=1, inplace=True)\n",
    "\n",
    "    a = df.shape[1]\n",
    "    df.drop(\"pop_20\", axis=1, inplace=True)\n",
    " \n",
    "\n",
    "    #recency_lapse can be converted to cat var, replace nan with \"no lapse\".\n",
    "        #can use bins of range 50 up to >400. \n",
    "        #Here we notice that for all the columns that have 11773 nan values, they refer to the 11773 people who do not have lapsed policies.\n",
    "    bins = [0, 50, 100, 150, 200, 250, 300, 350, 400, float('inf')]\n",
    "    labels = ['0-50', '51-100', '101-150', '151-200', '201-250', '251-300', '301-350', '351-400', '>400']\n",
    "    df['recency_lapse'] = pd.cut(df['recency_lapse'], bins=bins, labels=labels, right=False)\n",
    "    df['recency_lapse'] = df['recency_lapse'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    " \n",
    "        \n",
    "    #recency_cancel can be converted to cat var, replace nan with \"no cancel\".\n",
    "        #can use bins of range 20 up to >100. \n",
    "        #Similarly, we notice there are 16376 people who did not cancel any policy.\n",
    "    bins = [0, 20, 40, 60, 80, 100, float('inf')]\n",
    "    labels = ['0-20', '21-40', '41-60', '61-80', '81-100', '>100']\n",
    "    df['recency_cancel'] = pd.cut(df['recency_cancel'], bins=bins, labels=labels, right=False)\n",
    "    df['recency_cancel'] = df['recency_cancel'].cat.add_categories(\"No cancel\").fillna(\"No cancel\")\n",
    "\n",
    "    #tot_cancel_pols can change to 0 (stands for no policy cancelled).\n",
    "    df['tot_cancel_pols'].fillna(0, inplace=True)\n",
    "\n",
    "    #drop the lapse_ape columns that have less than 10 different values (nonzero, nonnull)\n",
    "    #for the others, convert to cat var (category for 0s; replace None with \"No lapse\")\n",
    "\n",
    "    #drop the n_months_since_lapse columns that have less than 10 different values (non-ve, non-9999, nonnull)\n",
    "    #for the others, convert to cat var (replace -ve, 9999 with \"No data\"; replace None with \"No lapse\")\n",
    "\n",
    "    #dropping the lapse_ape and n_months_since_lapse from above\n",
    "    cols = [\"lapse_ape_ltc_1280bf\",\"lapse_ape_grp_de05ae\",\"lapse_ape_inv_dcd836\",\"lapse_ape_lh_d0adeb\",\"lapse_ape_inv_e9f316\",\n",
    "        \"lapse_ape_32c74c\",\"n_months_since_lapse_ltc_1280bf\",\"n_months_since_lapse_grp_de05ae\",\"n_months_since_lapse_inv_dcd836\",\n",
    "        \"n_months_since_lapse_lh_d0adeb\",\"n_months_since_lapse_inv_e9f316\",\"n_months_since_lapse_32c74c\"]\n",
    "    a = df.shape[1]\n",
    "    df.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    #lapse_ape_grp_6fc3e6\n",
    "    bins = [0, 0.01, 100, 200, float('inf')]\n",
    "    labels = ['Zero', '>0-100', '101-200', '>200']\n",
    "    df['lapse_ape_grp_6fc3e6'].fillna(-1, inplace=True)\n",
    "    df['lapse_ape_grp_6fc3e6'] = pd.cut(df['lapse_ape_grp_6fc3e6'], bins=bins, labels=labels, right=False)\n",
    "    df['lapse_ape_grp_6fc3e6'] = df['lapse_ape_grp_6fc3e6'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "\n",
    "    #lapse_ape_grp_945b5a\n",
    "    bins = [0, 0.01, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, float('inf')]\n",
    "    labels = ['Zero', '>0-100', '101-200', '201-300', '301-400', '401-500', '501-600', '601-700', '701-800', '801-900', '901-1000', '>1000']\n",
    "    df['lapse_ape_grp_945b5a'].fillna(-1, inplace=True)\n",
    "    df['lapse_ape_grp_945b5a'] = pd.cut(df['lapse_ape_grp_945b5a'], bins=bins, labels=labels, right=False)\n",
    "    df['lapse_ape_grp_945b5a'] = df['lapse_ape_grp_945b5a'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "  \n",
    "    #lapse_ape_grp_6a5788 \n",
    "    bins = [0, 0.01, 200, 400, 600, float('inf')]\n",
    "    labels = ['Zero', '>0-200', '201-400', '401-600', '>600']\n",
    "    df['lapse_ape_grp_6a5788'].fillna(-1, inplace=True)\n",
    "    df['lapse_ape_grp_6a5788'] = pd.cut(df['lapse_ape_grp_6a5788'], bins=bins, labels=labels, right=False)\n",
    "    df['lapse_ape_grp_6a5788'] = df['lapse_ape_grp_6a5788'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "\n",
    "    #lapse_ape_ltc_43b9d5 \n",
    "    bins = [0, 0.01, 200, 400, 600, 800, 1000, float('inf')]\n",
    "    labels = ['Zero', '>0-200', '201-400', '401-600', '601-800', '801-1000', '>1000']\n",
    "    df['lapse_ape_ltc_43b9d5'].fillna(-1, inplace=True)\n",
    "    df['lapse_ape_ltc_43b9d5'] = pd.cut(df['lapse_ape_ltc_43b9d5'], bins=bins, labels=labels, right=False)\n",
    "    df['lapse_ape_ltc_43b9d5'] = df['lapse_ape_ltc_43b9d5'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "    \n",
    "    #lapse_ape_grp_9cdedf \n",
    "    bins = [0, 0.01, 100, 200, 300, 400, 500, float('inf')]\n",
    "    labels = ['Zero', '>0-100', '101-200', '201-300', '301-400', '401-500', '>500']\n",
    "    df['lapse_ape_grp_9cdedf'].fillna(-1, inplace=True)\n",
    "    df['lapse_ape_grp_9cdedf'] = pd.cut(df['lapse_ape_grp_9cdedf'], bins=bins, labels=labels, right=False)\n",
    "    df['lapse_ape_grp_9cdedf'] = df['lapse_ape_grp_9cdedf'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "\n",
    "    #lapse_ape_grp_1581d7\n",
    "    bins = [0, 0.01, 50, 100, 150, 200, 250, float('inf')]\n",
    "    labels = ['Zero', '>0-50', '51-100', '101-150', '151-200', '200-250', '>250']\n",
    "    df['lapse_ape_grp_1581d7'].fillna(-1, inplace=True)\n",
    "    df['lapse_ape_grp_1581d7'] = pd.cut(df['lapse_ape_grp_1581d7'], bins=bins, labels=labels, right=False)\n",
    "    df['lapse_ape_grp_1581d7'] = df['lapse_ape_grp_1581d7'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "   \n",
    "    #lapse_ape_grp_22decf\n",
    "    bins = [0, 0.01, 500, 1000, 1500, 2000, float('inf')]\n",
    "    labels = ['Zero', '>0-500', '501-1000', '1001-1500', '1501-2000','>2000']\n",
    "    df['lapse_ape_grp_22decf'].fillna(-1, inplace=True)\n",
    "    df['lapse_ape_grp_22decf'] = pd.cut(df['lapse_ape_grp_22decf'], bins=bins, labels=labels, right=False)\n",
    "    df['lapse_ape_grp_22decf'] = df['lapse_ape_grp_22decf'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "\n",
    "    #lapse_ape_lh_507c37\n",
    "    bins = [0, 0.01, 500, 1000, 1500, 2000, 3000, 4000, 5000, float('inf')]\n",
    "    labels = ['Zero', '>0-500', '501-1000', '1001-1500', '1501-2000','2001-3000','3001-4000','4001-5000','>5000']\n",
    "    df['lapse_ape_lh_507c37'].fillna(-1, inplace=True)\n",
    "    df['lapse_ape_lh_507c37'] = pd.cut(df['lapse_ape_lh_507c37'], bins=bins, labels=labels, right=False)\n",
    "    df['lapse_ape_lh_507c37'] = df['lapse_ape_lh_507c37'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    " \n",
    "    #lapse_ape_lh_839f8a\n",
    "    bins = [0, 0.01, 1000, 1500, 2000, 2500, 3000, 4000, 5000, float('inf')]\n",
    "    labels = ['Zero', '>0-1000', '1001-1500', '1501-2000', '2001-2500', '2501-3000', '3001-4000', '4001-5000', '>5000']\n",
    "    df['lapse_ape_lh_839f8a'].fillna(-1, inplace=True)\n",
    "    df['lapse_ape_lh_839f8a'] = pd.cut(df['lapse_ape_lh_839f8a'], bins=bins, labels=labels, right=False)\n",
    "    df['lapse_ape_lh_839f8a'] = df['lapse_ape_lh_839f8a'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "        \n",
    "    #lapse_ape_grp_caa6ff\n",
    "    bins = [0, 0.01, 100, 200, 300, 400, 500, 600, 700, float('inf')]\n",
    "    labels = ['Zero', '>0-100', '101-200', '201-300', '301-400', '401-500', '501-600', '601-700', '>700']\n",
    "    df['lapse_ape_grp_caa6ff'].fillna(-1, inplace=True)\n",
    "    df['lapse_ape_grp_caa6ff'] = pd.cut(df['lapse_ape_grp_caa6ff'], bins=bins, labels=labels, right=False)\n",
    "    df['lapse_ape_grp_caa6ff'] = df['lapse_ape_grp_caa6ff'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "\n",
    "    #lapse_ape_grp_fd3bfb\n",
    "    bins = [0, 0.01, 200, 400, 600, float('inf')]\n",
    "    labels = ['Zero', '>0-200', '201-400', '401-600', '>600']\n",
    "    df['lapse_ape_grp_fd3bfb'].fillna(-1, inplace=True)\n",
    "    df['lapse_ape_grp_fd3bfb'] = pd.cut(df['lapse_ape_grp_fd3bfb'], bins=bins, labels=labels, right=False)\n",
    "    df['lapse_ape_grp_fd3bfb'] = df['lapse_ape_grp_fd3bfb'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    " \n",
    "    #lapse_ape_lh_e22a6a\n",
    "    bins = [0, 0.01, 400, 800, 1200, 1600, 2000, 3000, 4000, float('inf')]\n",
    "    labels = ['Zero', '>0-400', '401-800', '801-1200', '1201-1600', '1601-2000', '2001-3000', '3001-4000', '>4000']\n",
    "    df['lapse_ape_lh_e22a6a'].fillna(-1, inplace=True)\n",
    "    df['lapse_ape_lh_e22a6a'] = pd.cut(df['lapse_ape_lh_e22a6a'], bins=bins, labels=labels, right=False)\n",
    "    df['lapse_ape_lh_e22a6a'] = df['lapse_ape_lh_e22a6a'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "  \n",
    "    #lapse_ape_grp_70e1dd\n",
    "    bins = [0, 0.01, 100, 200, 300, 400, 500, 600, float('inf')]\n",
    "    labels = ['Zero', '>0-100', '101-200', '201-300', '301-400', '401-500', '501-600', '>600']\n",
    "    df['lapse_ape_grp_70e1dd'].fillna(-1, inplace=True)\n",
    "    df['lapse_ape_grp_70e1dd'] = pd.cut(df['lapse_ape_grp_70e1dd'], bins=bins, labels=labels, right=False)\n",
    "    df['lapse_ape_grp_70e1dd'] = df['lapse_ape_grp_70e1dd'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "        \n",
    "    #lapse_ape_grp_e04c3a\n",
    "    bins = [0, 0.01, 500, 1000, 1500, 2000, float('inf')]\n",
    "    labels = ['Zero', '>0-500', '501-1000', '1001-1500', '1501-2000', '>2000']\n",
    "    df['lapse_ape_grp_e04c3a'].fillna(-1, inplace=True)\n",
    "    df['lapse_ape_grp_e04c3a'] = pd.cut(df['lapse_ape_grp_e04c3a'], bins=bins, labels=labels, right=False)\n",
    "    df['lapse_ape_grp_e04c3a'] = df['lapse_ape_grp_e04c3a'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "\n",
    "        \n",
    "    #lapse_ape_grp_fe5fb8\n",
    "    bins = [0, 0.01, 200, 400, 600, 800, 1000, 1200, 1400, 1600, float('inf')]\n",
    "    labels = ['Zero', '>0-200', '201-400', '401-600', '601-800', '801-1000', '1001-1200', '1201-1400', '1401-1600', '>1600']\n",
    "    df['lapse_ape_grp_fe5fb8'].fillna(-1, inplace=True)\n",
    "    df['lapse_ape_grp_fe5fb8'] = pd.cut(df['lapse_ape_grp_fe5fb8'], bins=bins, labels=labels, right=False)\n",
    "    df['lapse_ape_grp_fe5fb8'] = df['lapse_ape_grp_fe5fb8'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "\n",
    "    #lapse_ape_grp_94baec\n",
    "    bins = [0, 0.01, 100, 200, 400, 600, 800, 1000, float('inf')]\n",
    "    labels = ['Zero', '>0-100', '101-200', '201-400', '401-600', '601-800', '801-1000', '>1000']\n",
    "    df['lapse_ape_grp_94baec'].fillna(-1, inplace=True)\n",
    "    df['lapse_ape_grp_94baec'] = pd.cut(df['lapse_ape_grp_94baec'], bins=bins, labels=labels, right=False)\n",
    "    df['lapse_ape_grp_94baec'] = df['lapse_ape_grp_94baec'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "        \n",
    "    #lapse_ape_grp_e91421\n",
    "    bins = [0, 0.01, 200, 400, 600, 800, 1000, float('inf')]\n",
    "    labels = ['Zero', '>0-200', '201-400', '401-600', '601-800', '801-1000', '>1000']\n",
    "    df['lapse_ape_grp_e91421'].fillna(-1, inplace=True)\n",
    "    df['lapse_ape_grp_e91421'] = pd.cut(df['lapse_ape_grp_e91421'], bins=bins, labels=labels, right=False)\n",
    "    df['lapse_ape_grp_e91421'] = df['lapse_ape_grp_e91421'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "\n",
    "        \n",
    "    #lapse_ape_lh_f852af\n",
    "    bins = [0, 0.01, 500, 1000, 1500, 2000, 2500, 3000, 4000, 6000, 8000, float('inf')]\n",
    "    labels = ['Zero', '>0-500', '501-1000', '1001-1500', '1501-2000', '2001-2500', '2501-3000', '3001-4000', '4001-6000', '6001-8000', '>8000']\n",
    "    df['lapse_ape_lh_f852af'].fillna(-1, inplace=True)\n",
    "    df['lapse_ape_lh_f852af'] = pd.cut(df['lapse_ape_lh_f852af'], bins=bins, labels=labels, right=False)\n",
    "    df['lapse_ape_lh_f852af'] = df['lapse_ape_lh_f852af'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "\n",
    "        \n",
    "    #lapse_ape_lh_947b15\n",
    "    bins = [0, 0.01, 1000, 2000, 3000, 4000, 5000, 6000, float('inf')]\n",
    "    labels = ['Zero', '>0-1000', '1001-2000', '2001-3000', '3001-4000', '4001-5000', '5001-6000', '>6000']\n",
    "    df['lapse_ape_lh_947b15'].fillna(-1, inplace=True)\n",
    "    df['lapse_ape_lh_947b15'] = pd.cut(df['lapse_ape_lh_947b15'], bins=bins, labels=labels, right=False)\n",
    "    df['lapse_ape_lh_947b15'] = df['lapse_ape_lh_947b15'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "\n",
    "        \n",
    "    #converting n_months_since_lapse columns to cat var\n",
    "    cols=['n_months_since_lapse_grp_6fc3e6', 'n_months_since_lapse_grp_945b5a', 'n_months_since_lapse_grp_6a5788', \n",
    "    'n_months_since_lapse_ltc_43b9d5', 'n_months_since_lapse_grp_9cdedf',\n",
    "    'n_months_since_lapse_grp_1581d7', 'n_months_since_lapse_grp_22decf', 'n_months_since_lapse_lh_507c37', \n",
    "    'n_months_since_lapse_lh_839f8a', 'n_months_since_lapse_grp_caa6ff', \n",
    "    'n_months_since_lapse_grp_fd3bfb', 'n_months_since_lapse_lh_e22a6a', 'n_months_since_lapse_grp_70e1dd', \n",
    "    'n_months_since_lapse_grp_e04c3a', 'n_months_since_lapse_grp_fe5fb8', 'n_months_since_lapse_grp_94baec', \n",
    "    'n_months_since_lapse_grp_e91421', 'n_months_since_lapse_lh_f852af', 'n_months_since_lapse_lh_947b15']\n",
    "\n",
    "    bins = [0, 20, 40, 60, 80, 100, float('inf')]\n",
    "    labels = ['0-20', '21-40', '41-60', '61-80', '81-100', '>100']\n",
    "\n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype('float64').replace(9999,-1)\n",
    "        null_indices = df[col].index[df[col].isnull()].tolist()\n",
    "        df[col] = pd.cut(df[col], bins=bins, labels=labels, right=False)\n",
    "        df[col] = df[col].cat.add_categories(\"No lapse\")\n",
    "        df[col].loc[null_indices] = \"No lapse\"\n",
    "        df[col] = df[col].cat.add_categories(\"No data\").fillna(\"No data\")\n",
    "\n",
    "\n",
    "    #checking the distribution of columns, for determining the bins when converting to categorical variable\n",
    "\n",
    "    #flg_affconnect_show_interest_ever can change to False (1 change to True)\n",
    "    df['flg_affconnect_show_interest_ever'].fillna(False, inplace=True)\n",
    "    df['flg_affconnect_show_interest_ever'] = df['flg_affconnect_show_interest_ever'].astype(bool)\n",
    "\n",
    "\n",
    "    #flg_affconnect_ready_to_buy_ever can be dropped (very similar to n_months_since_visit_affcon)\n",
    "    a = df.shape[1]\n",
    "    df.drop(\"flg_affconnect_ready_to_buy_ever\", axis=1, inplace=True)\n",
    "   \n",
    "\n",
    "    #flg_affconnect_lapse_ever can be dropped (duplicate of have_lapse and flg_affconnect_ready_to_buy_ever)\n",
    "    a = df.shape[1]\n",
    "    df.drop(\"flg_affconnect_lapse_ever\", axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    #affcon_visit_days can be changed to 0\n",
    "        #Here we can hypothesise that 16176 people did not buy through affcon\n",
    "    df['affcon_visit_days'].fillna(0, inplace=True)\n",
    " \n",
    "        \n",
    "    #n_months_since_visit_affcon can be converted to cat var, replace nan with \"no affcon\".\n",
    "    df['n_months_since_visit_affcon'] = df['n_months_since_visit_affcon'].astype('category')\n",
    "    df['n_months_since_visit_affcon'] = df['n_months_since_visit_affcon'].cat.add_categories(\"No affcon\").fillna(\"No affcon\")\n",
    "\n",
    "\n",
    "    #clmcon_visit_days can be changed to 0\n",
    "        #Here we can hypothesise that 16279 people did not use clmcon\n",
    "    df['clmcon_visit_days'].fillna(0, inplace=True)\n",
    " \n",
    "        \n",
    "    #recency_clmcon, recency_clmcon_regis can be converted to cat var, replace nan with \"no clmcon\"\n",
    "        #recency_clmcon_regis can use bins of range 20\n",
    "    df['recency_clmcon'] = df['recency_clmcon'].astype('category')\n",
    "    df['recency_clmcon'] = df['recency_clmcon'].cat.add_categories(\"No clmcon\").fillna(\"No clmcon\")\n",
    "  \n",
    "\n",
    "    bins = [0, 10, 20, 30, 40, 60, 80, float('inf')]\n",
    "    labels = ['0-10', '11-20', '21-30', '31-40', '41-60', '61-80', '>80']\n",
    "    df['recency_clmcon_regis'] = pd.cut(df['recency_clmcon_regis'], bins=bins, labels=labels, right=False)\n",
    "    df['recency_clmcon_regis'] = df['recency_clmcon_regis'].cat.add_categories(\"No clmcon\").fillna(\"No clmcon\")\n",
    "    \n",
    "        \n",
    "    #hlthclaim_amt can be converted to cat var, replace nan with \"no hlthclaim\"\n",
    "        #Low: $0 - $500; Medium Low: $501 - $1,000; Medium: $1,001 - $5,000; High: $5,001 - $10,000; Very High: $10,001 and above\n",
    "        #Here we can hypothesise that 15552 people did not make healthclaims\n",
    "    bins = [0, 500, 1000, 5000, 10000, float('inf')]\n",
    "    labels = ['Low: $0 - $500', 'Medium Low: $501 - $1,000', 'Medium: $1,001 - $5,000', 'High: $5,001 - $10,000', 'Very High: >$10,000']\n",
    "    df['hlthclaim_amt'] = df['hlthclaim_amt'].fillna(-1)\n",
    "    df['hlthclaim_amt'] = pd.cut(df['hlthclaim_amt'], bins=bins, labels=labels, right=False)\n",
    "    df['hlthclaim_amt'] = df['hlthclaim_amt'].cat.add_categories(\"No healthclaim\").fillna(\"No healthclaim\")\n",
    "   \n",
    "        \n",
    "    #recency_hlthclaim can be converted to cat var, replace nan with \"no hlthclaim\"\n",
    "        #can use bins of range 20\n",
    "    bins = [0, 10, 20, 30, 40, 60, 80, 100, float('inf')]\n",
    "    labels = ['0-10', '11-20', '21-30', '31-40', '41-60', '61-80', '81-100', '>100']\n",
    "    df['recency_hlthclaim'] = pd.cut(df['recency_hlthclaim'], bins=bins, labels=labels, right=False)\n",
    "    df['recency_hlthclaim'] = df['recency_hlthclaim'].cat.add_categories(\"No healthclaim\").fillna(\"No healthclaim\")\n",
    " \n",
    "    #hlthclaim_cnt_success can be converted to cat var, replace nan with \"no success\"\n",
    "        #can use bins of ranges 0-20; 21-40; 41-100; 101-180; >180\n",
    "    bins = [0, 20, 40, 100, 180, float('inf')]\n",
    "    labels = ['0-20', '21-40', '41-100', '101-180', '>180']\n",
    "    df['hlthclaim_cnt_success'] = pd.cut(df['hlthclaim_cnt_success'], bins=bins, labels=labels, right=False)\n",
    "    df['hlthclaim_cnt_success'] = df['hlthclaim_cnt_success'].cat.add_categories(\"No success\").fillna(\"No success\")\n",
    "  \n",
    "    #recency_hlthclaim_success can be converted to cat var, replace nan with \"no success\"\n",
    "        #can use bins of range 20\n",
    "    bins = [0, 10, 20, 30, 40, 60, 80, 100, float('inf')]\n",
    "    labels = ['0-10', '11-20', '21-30', '31-40', '41-60', '61-80', '81-100', '>100']\n",
    "    df['recency_hlthclaim_success'] = pd.cut(df['recency_hlthclaim_success'], bins=bins, labels=labels, right=False)\n",
    "    df['recency_hlthclaim_success'] = df['recency_hlthclaim_success'].cat.add_categories(\"No success\").fillna(\"No success\")\n",
    "    \n",
    "        \n",
    "    #hlthclaim_cnt_unsuccess can be converted to cat var, replace nan with \"no unsuccess\"\n",
    "    bins = [0, 1, 2, 4, 6, 8, 10, 15, float('inf')]\n",
    "    labels = ['1', '2', '3-4', '5-6', '7-8', '9-10', '11-15', '>15']\n",
    "    df['hlthclaim_cnt_unsuccess'] = pd.cut(df['hlthclaim_cnt_unsuccess'], bins=bins, labels=labels, right=False)\n",
    "    df['hlthclaim_cnt_unsuccess'] = df['hlthclaim_cnt_unsuccess'].cat.add_categories(\"No unsuccess\").fillna(\"No unsuccess\")\n",
    " \n",
    "        \n",
    "    #recency_hlthclaim_unsuccess can be converted to cat var, replace nan with \"no unsuccess\"\n",
    "        #can use bins of range 20\n",
    "    bins = [0, 20, 40, 60, 80, 100, 120, float('inf')]\n",
    "    labels = ['0-20', '21-40', '41-60', '61-80', '81-100', '101-120', '>120']\n",
    "    df['recency_hlthclaim_unsuccess'] = pd.cut(df['recency_hlthclaim_unsuccess'], bins=bins, labels=labels, right=False)\n",
    "    df['recency_hlthclaim_unsuccess'] = df['recency_hlthclaim_unsuccess'].cat.add_categories(\"No unsuccess\").fillna(\"No unsuccess\")\n",
    "  \n",
    "\n",
    "    #flg_hlthclaim_839f8a_ever can change to False (1 change to True)\n",
    "    df['flg_hlthclaim_839f8a_ever'].fillna(False, inplace=True)\n",
    "    df['flg_hlthclaim_839f8a_ever'] = df['flg_hlthclaim_839f8a_ever'].astype(bool)\n",
    "\n",
    "\n",
    "    #recency_hlthclaim_839f8a can be converted to cat var, replace nan with \"no claim\"\n",
    "    bins = [0, 2, 5, 10, 20, 40, 60, 80, 100, 120, float('inf')]\n",
    "    labels = ['0-2', '3-5', '6-10', '11-20', '21-40', '41-60', '61-80', '81-100', '101-120', '>120']\n",
    "    df['recency_hlthclaim_839f8a'] = pd.cut(df['recency_hlthclaim_839f8a'], bins=bins, labels=labels, right=False)\n",
    "    df['recency_hlthclaim_839f8a'] = df['recency_hlthclaim_839f8a'].cat.add_categories(\"No claim\").fillna(\"No claim\")\n",
    "\n",
    "        \n",
    "    #flg_hlthclaim_14cb37_ever can change to False (1 change to True)\n",
    "    df['flg_hlthclaim_14cb37_ever'].fillna(False, inplace=True)\n",
    "    df['flg_hlthclaim_14cb37_ever'] = df['flg_hlthclaim_14cb37_ever'].astype(bool)\n",
    "\n",
    "\n",
    "    #recency_hlthclaim_14cb37 can be converted to cat var, replace nan with \"no claim\"\n",
    "    bins = [0, 2, 5, 10, 20, 40, 60, 80, 100, 120, float('inf')]\n",
    "    labels = ['0-2', '3-5', '6-10', '11-20', '21-40', '41-60', '61-80', '81-100', '101-120', '>120']\n",
    "    df['recency_hlthclaim_14cb37'] = pd.cut(df['recency_hlthclaim_14cb37'], bins=bins, labels=labels, right=False)\n",
    "    df['recency_hlthclaim_14cb37'] = df['recency_hlthclaim_14cb37'].cat.add_categories(\"No claim\").fillna(\"No claim\")\n",
    "\n",
    "        \n",
    "    #giclaim_amt can be converted to cat var, replace nan with \"no giclaim\"\n",
    "        #Low: $0 - $500; Medium Low: $501 - $1,000; Medium: $1,001 - $5,000; High: $5,001 - $10,000; Very High: $10,001 and above\n",
    "        #Here we can hypothesise that 16545 people did not make giclaims\n",
    "    bins = [0, 500, 1000, 5000, 10000, float('inf')]\n",
    "    labels = ['Low: $0 - $500', 'Medium Low: $501 - $1,000', 'Medium: $1,001 - $5,000', 'High: $5,001 - $10,000', 'Very High: >$10,000']\n",
    "    df['giclaim_amt'] = df['giclaim_amt'].fillna(-1)\n",
    "    df['giclaim_amt'] = pd.cut(df['giclaim_amt'], bins=bins, labels=labels, right=False)\n",
    "    df['giclaim_amt'] = df['giclaim_amt'].cat.add_categories(\"No giclaim\").fillna(\"No giclaim\")\n",
    "   \n",
    "        \n",
    "    #recency_giclaim can be converted to cat var, replace nan with \"no giclaim\"\n",
    "        #can use bins of range 10\n",
    "    bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, float('inf')]\n",
    "    labels = ['0-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '>100']\n",
    "    df['recency_giclaim'] = pd.cut(df['recency_giclaim'], bins=bins, labels=labels, right=False)\n",
    "    df['recency_giclaim'] = df['recency_giclaim'].cat.add_categories(\"No giclaim\").fillna(\"No giclaim\")\n",
    "    \n",
    "   # Function to convert object columns to categorical\n",
    "    def convert_to_categorical(df):\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                df[col] = df[col].astype('category')\n",
    "        return df\n",
    "    \n",
    "    ## Export model\n",
    "    RF_Model = h2o.load_model('./rfTrainedModel')\n",
    "\n",
    "    df = convert_to_categorical(df)\n",
    "\n",
    "    result = RF_Model.predict(h2o.H2OFrame(df))\n",
    "    return result['predict']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell to check testing_hidden_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should output a list of predictions.\n",
    "test_df = pd.read_parquet(filepath)\n",
    "test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
    "print(testing_hidden_data(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The below code block is for training purposes and analysis ONLY. It does not need to be run for obtaining the prediction results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning (dealing with nan values)\n",
    "\n",
    "1. Check for and eliminate duplicate rows.\n",
    "2. Drop id column.\n",
    "3. Drop columns with only one value.\n",
    "4. Drop uninformative samples (missing the 20 key features).\n",
    "5. Fix null values column by column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Search for duplicates\n",
    "a = df[\"clntnum\"].nunique()\n",
    "if a == 17992:\n",
    "    print(\"There are no duplicates.\")\n",
    "else:\n",
    "    b = 17992 - a\n",
    "    print(f\"There are {b} duplicates.\")\n",
    "\n",
    "\n",
    "\n",
    "#2. Drop \"clntnum\" column\n",
    "df.drop(\"clntnum\", axis=1, inplace=True)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "\n",
    "#3. Drop columns that have only one value\n",
    "for col in df:\n",
    "    if df[col].nunique(dropna=False) == 1:\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "print(df.shape)         #57 columns dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Drop the 1014 rows with NaN for the 20 key columns (not very informative samples)\n",
    "df = df[~df[\"flg_substandard\"].isna()]\n",
    "print(df.shape)\n",
    "\n",
    "#Studying the columns with null values\n",
    "for column in df.columns:\n",
    "    if df[column].isnull().any():\n",
    "        p = df[column].isnull().sum()/16978*100\n",
    "        unique_values = df[column].unique()\n",
    "        print(f\"{column} has {p}% null values. Unique values: {unique_values}\")\n",
    "        \n",
    "#imputation by taking a random sample from non-null values in the column\n",
    "def impute(df, column_to_impute):\n",
    "    non_null_values = df[column_to_impute].dropna().values\n",
    "    null_indices = df[df[column_to_impute].isnull()].index\n",
    "    df.loc[null_indices, column_to_impute] = np.random.choice(non_null_values, size=len(null_indices), replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#race_desc can use imputation by random sampling from non-null values.\n",
    "impute(df, \"race_desc\")\n",
    "if df[\"race_desc\"].isnull().sum() == 0:\n",
    "    print(\"race_desc done\")\n",
    "\n",
    "#ctrycode_desc can change to \"Not Applicable\".\n",
    "df['ctrycode_desc'].fillna(\"Not Applicable\", inplace=True)\n",
    "if df[\"ctrycode_desc\"].isnull().sum() == 0:\n",
    "    print(\"ctrycode_desc done\")\n",
    "\n",
    "#cltsex_fix can use imputation.\n",
    "impute(df, \"cltsex_fix\")\n",
    "if df[\"cltsex_fix\"].isnull().sum() == 0:\n",
    "    print(\"cltsex_fix done\")\n",
    "\n",
    "#f_ever_declined_la can change to False (1 change to True)\n",
    "df['f_ever_declined_la'].fillna(False, inplace=True)\n",
    "df['f_ever_declined_la'] = df['f_ever_declined_la'].astype(bool)\n",
    "if df[\"f_ever_declined_la\"].isnull().sum() == 0:\n",
    "    print(\"f_ever_declined_la done\")\n",
    "\n",
    "#hh_size can be dropped (duplicate of hh_size_est)\n",
    "a = df.shape[1]\n",
    "df.drop(\"hh_size\", axis=1, inplace=True)\n",
    "if df.shape[1] + 1 == a:\n",
    "    print(\"Dropped hh_size\")\n",
    "print(df.shape)\n",
    "\n",
    "#hh_size_est can use imputation.\n",
    "impute(df, \"hh_size_est\")\n",
    "if df[\"hh_size_est\"].isnull().sum() == 0:\n",
    "    print(\"hh_size_est done\")\n",
    "\n",
    "#annual_income_est can use imputation.\n",
    "impute(df, \"annual_income_est\")\n",
    "if df[\"annual_income_est\"].isnull().sum() == 0:\n",
    "    print(\"annual_income_est done\")\n",
    "    \n",
    "#dk what the col mean: hh_20, pop_20 (just drop)\n",
    "a = df.shape[1]\n",
    "df.drop(\"hh_20\", axis=1, inplace=True)\n",
    "if df.shape[1] + 1 == a:\n",
    "    print(\"Dropped hh_20\")\n",
    "print(df.shape)\n",
    "\n",
    "a = df.shape[1]\n",
    "df.drop(\"pop_20\", axis=1, inplace=True)\n",
    "if df.shape[1] + 1 == a:\n",
    "    print(\"Dropped pop_20\")\n",
    "print(df.shape)\n",
    "\n",
    "#recency_lapse can be converted to cat var, replace nan with \"no lapse\".\n",
    "    #can use bins of range 50 up to >400. \n",
    "    #Here we notice that for all the columns that have 11773 nan values, they refer to the 11773 people who do not have lapsed policies.\n",
    "bins = [0, 50, 100, 150, 200, 250, 300, 350, 400, float('inf')]\n",
    "labels = ['0-50', '51-100', '101-150', '151-200', '201-250', '251-300', '301-350', '351-400', '>400']\n",
    "df['recency_lapse'] = pd.cut(df['recency_lapse'], bins=bins, labels=labels, right=False)\n",
    "df['recency_lapse'] = df['recency_lapse'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "if df[\"recency_lapse\"].isnull().sum() == 0:\n",
    "    print(\"recency_lapse done\")\n",
    "    \n",
    "#recency_cancel can be converted to cat var, replace nan with \"no cancel\".\n",
    "    #can use bins of range 20 up to >100. \n",
    "    #Similarly, we notice there are 16376 people who did not cancel any policy.\n",
    "bins = [0, 20, 40, 60, 80, 100, float('inf')]\n",
    "labels = ['0-20', '21-40', '41-60', '61-80', '81-100', '>100']\n",
    "df['recency_cancel'] = pd.cut(df['recency_cancel'], bins=bins, labels=labels, right=False)\n",
    "df['recency_cancel'] = df['recency_cancel'].cat.add_categories(\"No cancel\").fillna(\"No cancel\")\n",
    "if df[\"recency_cancel\"].isnull().sum() == 0:\n",
    "    print(\"recency_cancel done\")\n",
    "    \n",
    "#tot_cancel_pols can change to 0 (stands for no policy cancelled).\n",
    "df['tot_cancel_pols'].fillna(0, inplace=True)\n",
    "if df[\"tot_cancel_pols\"].isnull().sum() == 0:\n",
    "    print(\"tot_cancel_pols done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other lapse features: lapse_ape_ltc_1280bf, lapse_ape_grp_6fc3e6, lapse_ape_grp_de05ae, lapse_ape_inv_dcd836, lapse_ape_grp_945b5a, \n",
    "#                      lapse_ape_grp_6a5788, lapse_ape_ltc_43b9d5, lapse_ape_grp_9cdedf, lapse_ape_lh_d0adeb, lapse_ape_grp_1581d7, \n",
    "#                      lapse_ape_grp_22decf, lapse_ape_lh_507c37, lapse_ape_lh_839f8a, lapse_ape_inv_e9f316, lapse_ape_grp_caa6ff, \n",
    "#                      lapse_ape_grp_fd3bfb, lapse_ape_lh_e22a6a, lapse_ape_grp_70e1dd, lapse_ape_grp_e04c3a, lapse_ape_grp_fe5fb8, \n",
    "#                      lapse_ape_grp_94baec, lapse_ape_grp_e91421, lapse_ape_lh_f852af, lapse_ape_lh_947b15, lapse_ape_32c74c, \n",
    "#                      n_months_since_lapse_ltc_1280bf, n_months_since_lapse_grp_6fc3e6, n_months_since_lapse_grp_de05ae, \n",
    "#                      n_months_since_lapse_inv_dcd836, n_months_since_lapse_grp_945b5a, n_months_since_lapse_grp_6a5788, \n",
    "#                      n_months_since_lapse_ltc_43b9d5, n_months_since_lapse_grp_9cdedf, n_months_since_lapse_lh_d0adeb, \n",
    "#                      n_months_since_lapse_grp_1581d7, n_months_since_lapse_grp_22decf, n_months_since_lapse_lh_507c37, \n",
    "#                      n_months_since_lapse_lh_839f8a, n_months_since_lapse_inv_e9f316, n_months_since_lapse_grp_caa6ff, \n",
    "#                      n_months_since_lapse_grp_fd3bfb, n_months_since_lapse_lh_e22a6a, n_months_since_lapse_grp_70e1dd, \n",
    "#                      n_months_since_lapse_grp_e04c3a, n_months_since_lapse_grp_fe5fb8, n_months_since_lapse_grp_94baec, \n",
    "#                      n_months_since_lapse_grp_e91421, n_months_since_lapse_lh_f852af, n_months_since_lapse_lh_947b15, \n",
    "#                      n_months_since_lapse_32c74c\n",
    "\n",
    "#drop the lapse_ape columns that have less than 10 different values (nonzero, nonnull)\n",
    "#drop lapse_ape_ltc_1280bf (0), lapse_ape_grp_de05ae (1), lapse_ape_inv_dcd836 (0), lapse_ape_lh_d0adeb (0),\n",
    "#     lapse_ape_inv_e9f316 (5), lapse_ape_32c74c (0)\n",
    "#for the others, convert to cat var (category for 0s; replace None with \"No lapse\")\n",
    "\n",
    "#drop the n_months_since_lapse columns that have less than 10 different values (non-ve, non-9999, nonnull)\n",
    "#drop n_months_since_lapse_ltc_1280bf (0), n_months_since_lapse_grp_de05ae (2), n_months_since_lapse_inv_dcd836 (0),\n",
    "#     n_months_since_lapse_lh_d0adeb (0), n_months_since_lapse_inv_e9f316 (7), n_months_since_lapse_32c74c (0)\n",
    "#for the others, convert to cat var (replace -ve, 9999 with \"No data\"; replace None with \"No lapse\")\n",
    "\n",
    "#dropping the lapse_ape and n_months_since_lapse from above\n",
    "cols = [\"lapse_ape_ltc_1280bf\",\"lapse_ape_grp_de05ae\",\"lapse_ape_inv_dcd836\",\"lapse_ape_lh_d0adeb\",\"lapse_ape_inv_e9f316\",\n",
    "       \"lapse_ape_32c74c\",\"n_months_since_lapse_ltc_1280bf\",\"n_months_since_lapse_grp_de05ae\",\"n_months_since_lapse_inv_dcd836\",\n",
    "       \"n_months_since_lapse_lh_d0adeb\",\"n_months_since_lapse_inv_e9f316\",\"n_months_since_lapse_32c74c\"]\n",
    "a = df.shape[1]\n",
    "df.drop(cols, axis=1, inplace=True)\n",
    "if df.shape[1] + 12 == a:\n",
    "    print(\"Dropped 12 columns\")\n",
    "print(df.shape)\n",
    "\n",
    "#lapse_ape_grp_6fc3e6\n",
    "bins = [0, 0.01, 100, 200, float('inf')]\n",
    "labels = ['Zero', '>0-100', '101-200', '>200']\n",
    "df['lapse_ape_grp_6fc3e6'].fillna(-1, inplace=True)\n",
    "df['lapse_ape_grp_6fc3e6'] = pd.cut(df['lapse_ape_grp_6fc3e6'], bins=bins, labels=labels, right=False)\n",
    "df['lapse_ape_grp_6fc3e6'] = df['lapse_ape_grp_6fc3e6'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "if df[\"lapse_ape_grp_6fc3e6\"].isnull().sum() == 0:\n",
    "    print(\"lapse_ape_grp_6fc3e6 done\")\n",
    "\n",
    "#lapse_ape_grp_945b5a\n",
    "bins = [0, 0.01, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, float('inf')]\n",
    "labels = ['Zero', '>0-100', '101-200', '201-300', '301-400', '401-500', '501-600', '601-700', '701-800', '801-900', '901-1000', '>1000']\n",
    "df['lapse_ape_grp_945b5a'].fillna(-1, inplace=True)\n",
    "df['lapse_ape_grp_945b5a'] = pd.cut(df['lapse_ape_grp_945b5a'], bins=bins, labels=labels, right=False)\n",
    "df['lapse_ape_grp_945b5a'] = df['lapse_ape_grp_945b5a'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "if df[\"lapse_ape_grp_945b5a\"].isnull().sum() == 0:\n",
    "    print(\"lapse_ape_grp_945b5a done\")\n",
    "\n",
    "#lapse_ape_grp_6a5788 \n",
    "bins = [0, 0.01, 200, 400, 600, float('inf')]\n",
    "labels = ['Zero', '>0-200', '201-400', '401-600', '>600']\n",
    "df['lapse_ape_grp_6a5788'].fillna(-1, inplace=True)\n",
    "df['lapse_ape_grp_6a5788'] = pd.cut(df['lapse_ape_grp_6a5788'], bins=bins, labels=labels, right=False)\n",
    "df['lapse_ape_grp_6a5788'] = df['lapse_ape_grp_6a5788'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "if df[\"lapse_ape_grp_6a5788\"].isnull().sum() == 0:\n",
    "    print(\"lapse_ape_grp_6a5788 done\")\n",
    "    \n",
    "#lapse_ape_ltc_43b9d5 \n",
    "bins = [0, 0.01, 200, 400, 600, 800, 1000, float('inf')]\n",
    "labels = ['Zero', '>0-200', '201-400', '401-600', '601-800', '801-1000', '>1000']\n",
    "df['lapse_ape_ltc_43b9d5'].fillna(-1, inplace=True)\n",
    "df['lapse_ape_ltc_43b9d5'] = pd.cut(df['lapse_ape_ltc_43b9d5'], bins=bins, labels=labels, right=False)\n",
    "df['lapse_ape_ltc_43b9d5'] = df['lapse_ape_ltc_43b9d5'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "if df[\"lapse_ape_ltc_43b9d5\"].isnull().sum() == 0:\n",
    "    print(\"lapse_ape_ltc_43b9d5 done\")\n",
    "    \n",
    "#lapse_ape_grp_9cdedf \n",
    "bins = [0, 0.01, 100, 200, 300, 400, 500, float('inf')]\n",
    "labels = ['Zero', '>0-100', '101-200', '201-300', '301-400', '401-500', '>500']\n",
    "df['lapse_ape_grp_9cdedf'].fillna(-1, inplace=True)\n",
    "df['lapse_ape_grp_9cdedf'] = pd.cut(df['lapse_ape_grp_9cdedf'], bins=bins, labels=labels, right=False)\n",
    "df['lapse_ape_grp_9cdedf'] = df['lapse_ape_grp_9cdedf'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "if df[\"lapse_ape_grp_9cdedf\"].isnull().sum() == 0:\n",
    "    print(\"lapse_ape_grp_9cdedf done\")\n",
    "    \n",
    "#lapse_ape_grp_1581d7\n",
    "bins = [0, 0.01, 50, 100, 150, 200, 250, float('inf')]\n",
    "labels = ['Zero', '>0-50', '51-100', '101-150', '151-200', '200-250', '>250']\n",
    "df['lapse_ape_grp_1581d7'].fillna(-1, inplace=True)\n",
    "df['lapse_ape_grp_1581d7'] = pd.cut(df['lapse_ape_grp_1581d7'], bins=bins, labels=labels, right=False)\n",
    "df['lapse_ape_grp_1581d7'] = df['lapse_ape_grp_1581d7'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "if df[\"lapse_ape_grp_1581d7\"].isnull().sum() == 0:\n",
    "    print(\"lapse_ape_grp_1581d7 done\")\n",
    "    \n",
    "#lapse_ape_grp_22decf\n",
    "bins = [0, 0.01, 500, 1000, 1500, 2000, float('inf')]\n",
    "labels = ['Zero', '>0-500', '501-1000', '1001-1500', '1501-2000','>2000']\n",
    "df['lapse_ape_grp_22decf'].fillna(-1, inplace=True)\n",
    "df['lapse_ape_grp_22decf'] = pd.cut(df['lapse_ape_grp_22decf'], bins=bins, labels=labels, right=False)\n",
    "df['lapse_ape_grp_22decf'] = df['lapse_ape_grp_22decf'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "if df[\"lapse_ape_grp_22decf\"].isnull().sum() == 0:\n",
    "    print(\"lapse_ape_grp_22decf done\")\n",
    "    \n",
    "#lapse_ape_lh_507c37\n",
    "bins = [0, 0.01, 500, 1000, 1500, 2000, 3000, 4000, 5000, float('inf')]\n",
    "labels = ['Zero', '>0-500', '501-1000', '1001-1500', '1501-2000','2001-3000','3001-4000','4001-5000','>5000']\n",
    "df['lapse_ape_lh_507c37'].fillna(-1, inplace=True)\n",
    "df['lapse_ape_lh_507c37'] = pd.cut(df['lapse_ape_lh_507c37'], bins=bins, labels=labels, right=False)\n",
    "df['lapse_ape_lh_507c37'] = df['lapse_ape_lh_507c37'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "if df[\"lapse_ape_lh_507c37\"].isnull().sum() == 0:\n",
    "    print(\"lapse_ape_lh_507c37 done\")\n",
    "\n",
    "#lapse_ape_lh_839f8a\n",
    "bins = [0, 0.01, 1000, 1500, 2000, 2500, 3000, 4000, 5000, float('inf')]\n",
    "labels = ['Zero', '>0-1000', '1001-1500', '1501-2000', '2001-2500', '2501-3000', '3001-4000', '4001-5000', '>5000']\n",
    "df['lapse_ape_lh_839f8a'].fillna(-1, inplace=True)\n",
    "df['lapse_ape_lh_839f8a'] = pd.cut(df['lapse_ape_lh_839f8a'], bins=bins, labels=labels, right=False)\n",
    "df['lapse_ape_lh_839f8a'] = df['lapse_ape_lh_839f8a'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "if df[\"lapse_ape_lh_839f8a\"].isnull().sum() == 0:\n",
    "    print(\"lapse_ape_lh_839f8a done\")\n",
    "    \n",
    "#lapse_ape_grp_caa6ff\n",
    "bins = [0, 0.01, 100, 200, 300, 400, 500, 600, 700, float('inf')]\n",
    "labels = ['Zero', '>0-100', '101-200', '201-300', '301-400', '401-500', '501-600', '601-700', '>700']\n",
    "df['lapse_ape_grp_caa6ff'].fillna(-1, inplace=True)\n",
    "df['lapse_ape_grp_caa6ff'] = pd.cut(df['lapse_ape_grp_caa6ff'], bins=bins, labels=labels, right=False)\n",
    "df['lapse_ape_grp_caa6ff'] = df['lapse_ape_grp_caa6ff'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "if df[\"lapse_ape_grp_caa6ff\"].isnull().sum() == 0:\n",
    "    print(\"lapse_ape_grp_caa6ff done\")\n",
    "    \n",
    "#lapse_ape_grp_fd3bfb\n",
    "bins = [0, 0.01, 200, 400, 600, float('inf')]\n",
    "labels = ['Zero', '>0-200', '201-400', '401-600', '>600']\n",
    "df['lapse_ape_grp_fd3bfb'].fillna(-1, inplace=True)\n",
    "df['lapse_ape_grp_fd3bfb'] = pd.cut(df['lapse_ape_grp_fd3bfb'], bins=bins, labels=labels, right=False)\n",
    "df['lapse_ape_grp_fd3bfb'] = df['lapse_ape_grp_fd3bfb'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "if df[\"lapse_ape_grp_fd3bfb\"].isnull().sum() == 0:\n",
    "    print(\"lapse_ape_grp_fd3bfb done\")\n",
    "    \n",
    "#lapse_ape_lh_e22a6a\n",
    "bins = [0, 0.01, 400, 800, 1200, 1600, 2000, 3000, 4000, float('inf')]\n",
    "labels = ['Zero', '>0-400', '401-800', '801-1200', '1201-1600', '1601-2000', '2001-3000', '3001-4000', '>4000']\n",
    "df['lapse_ape_lh_e22a6a'].fillna(-1, inplace=True)\n",
    "df['lapse_ape_lh_e22a6a'] = pd.cut(df['lapse_ape_lh_e22a6a'], bins=bins, labels=labels, right=False)\n",
    "df['lapse_ape_lh_e22a6a'] = df['lapse_ape_lh_e22a6a'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "if df[\"lapse_ape_lh_e22a6a\"].isnull().sum() == 0:\n",
    "    print(\"lapse_ape_lh_e22a6a done\")\n",
    "    \n",
    "#lapse_ape_grp_70e1dd\n",
    "bins = [0, 0.01, 100, 200, 300, 400, 500, 600, float('inf')]\n",
    "labels = ['Zero', '>0-100', '101-200', '201-300', '301-400', '401-500', '501-600', '>600']\n",
    "df['lapse_ape_grp_70e1dd'].fillna(-1, inplace=True)\n",
    "df['lapse_ape_grp_70e1dd'] = pd.cut(df['lapse_ape_grp_70e1dd'], bins=bins, labels=labels, right=False)\n",
    "df['lapse_ape_grp_70e1dd'] = df['lapse_ape_grp_70e1dd'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "if df[\"lapse_ape_grp_70e1dd\"].isnull().sum() == 0:\n",
    "    print(\"lapse_ape_grp_70e1dd done\")\n",
    "    \n",
    "#lapse_ape_grp_e04c3a\n",
    "bins = [0, 0.01, 500, 1000, 1500, 2000, float('inf')]\n",
    "labels = ['Zero', '>0-500', '501-1000', '1001-1500', '1501-2000', '>2000']\n",
    "df['lapse_ape_grp_e04c3a'].fillna(-1, inplace=True)\n",
    "df['lapse_ape_grp_e04c3a'] = pd.cut(df['lapse_ape_grp_e04c3a'], bins=bins, labels=labels, right=False)\n",
    "df['lapse_ape_grp_e04c3a'] = df['lapse_ape_grp_e04c3a'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "if df[\"lapse_ape_grp_e04c3a\"].isnull().sum() == 0:\n",
    "    print(\"lapse_ape_grp_e04c3a done\")\n",
    "    \n",
    "#lapse_ape_grp_fe5fb8\n",
    "bins = [0, 0.01, 200, 400, 600, 800, 1000, 1200, 1400, 1600, float('inf')]\n",
    "labels = ['Zero', '>0-200', '201-400', '401-600', '601-800', '801-1000', '1001-1200', '1201-1400', '1401-1600', '>1600']\n",
    "df['lapse_ape_grp_fe5fb8'].fillna(-1, inplace=True)\n",
    "df['lapse_ape_grp_fe5fb8'] = pd.cut(df['lapse_ape_grp_fe5fb8'], bins=bins, labels=labels, right=False)\n",
    "df['lapse_ape_grp_fe5fb8'] = df['lapse_ape_grp_fe5fb8'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "if df[\"lapse_ape_grp_fe5fb8\"].isnull().sum() == 0:\n",
    "    print(\"lapse_ape_grp_fe5fb8 done\")\n",
    "    \n",
    "#lapse_ape_grp_94baec\n",
    "bins = [0, 0.01, 100, 200, 400, 600, 800, 1000, float('inf')]\n",
    "labels = ['Zero', '>0-100', '101-200', '201-400', '401-600', '601-800', '801-1000', '>1000']\n",
    "df['lapse_ape_grp_94baec'].fillna(-1, inplace=True)\n",
    "df['lapse_ape_grp_94baec'] = pd.cut(df['lapse_ape_grp_94baec'], bins=bins, labels=labels, right=False)\n",
    "df['lapse_ape_grp_94baec'] = df['lapse_ape_grp_94baec'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "if df[\"lapse_ape_grp_94baec\"].isnull().sum() == 0:\n",
    "    print(\"lapse_ape_grp_94baec done\")\n",
    "    \n",
    "#lapse_ape_grp_e91421\n",
    "bins = [0, 0.01, 200, 400, 600, 800, 1000, float('inf')]\n",
    "labels = ['Zero', '>0-200', '201-400', '401-600', '601-800', '801-1000', '>1000']\n",
    "df['lapse_ape_grp_e91421'].fillna(-1, inplace=True)\n",
    "df['lapse_ape_grp_e91421'] = pd.cut(df['lapse_ape_grp_e91421'], bins=bins, labels=labels, right=False)\n",
    "df['lapse_ape_grp_e91421'] = df['lapse_ape_grp_e91421'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "if df[\"lapse_ape_grp_e91421\"].isnull().sum() == 0:\n",
    "    print(\"lapse_ape_grp_e91421 done\")\n",
    "    \n",
    "#lapse_ape_lh_f852af\n",
    "bins = [0, 0.01, 500, 1000, 1500, 2000, 2500, 3000, 4000, 6000, 8000, float('inf')]\n",
    "labels = ['Zero', '>0-500', '501-1000', '1001-1500', '1501-2000', '2001-2500', '2501-3000', '3001-4000', '4001-6000', '6001-8000', '>8000']\n",
    "df['lapse_ape_lh_f852af'].fillna(-1, inplace=True)\n",
    "df['lapse_ape_lh_f852af'] = pd.cut(df['lapse_ape_lh_f852af'], bins=bins, labels=labels, right=False)\n",
    "df['lapse_ape_lh_f852af'] = df['lapse_ape_lh_f852af'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "if df[\"lapse_ape_lh_f852af\"].isnull().sum() == 0:\n",
    "    print(\"lapse_ape_lh_f852af done\")\n",
    "    \n",
    "#lapse_ape_lh_947b15\n",
    "bins = [0, 0.01, 1000, 2000, 3000, 4000, 5000, 6000, float('inf')]\n",
    "labels = ['Zero', '>0-1000', '1001-2000', '2001-3000', '3001-4000', '4001-5000', '5001-6000', '>6000']\n",
    "df['lapse_ape_lh_947b15'].fillna(-1, inplace=True)\n",
    "df['lapse_ape_lh_947b15'] = pd.cut(df['lapse_ape_lh_947b15'], bins=bins, labels=labels, right=False)\n",
    "df['lapse_ape_lh_947b15'] = df['lapse_ape_lh_947b15'].cat.add_categories(\"No lapse\").fillna(\"No lapse\")\n",
    "if df[\"lapse_ape_lh_947b15\"].isnull().sum() == 0:\n",
    "    print(\"lapse_ape_lh_947b15 done\")\n",
    "    \n",
    "#converting n_months_since_lapse columns to cat var\n",
    "cols=['n_months_since_lapse_grp_6fc3e6', 'n_months_since_lapse_grp_945b5a', 'n_months_since_lapse_grp_6a5788', \n",
    "'n_months_since_lapse_ltc_43b9d5', 'n_months_since_lapse_grp_9cdedf',\n",
    "'n_months_since_lapse_grp_1581d7', 'n_months_since_lapse_grp_22decf', 'n_months_since_lapse_lh_507c37', \n",
    "'n_months_since_lapse_lh_839f8a', 'n_months_since_lapse_grp_caa6ff', \n",
    "'n_months_since_lapse_grp_fd3bfb', 'n_months_since_lapse_lh_e22a6a', 'n_months_since_lapse_grp_70e1dd', \n",
    "'n_months_since_lapse_grp_e04c3a', 'n_months_since_lapse_grp_fe5fb8', 'n_months_since_lapse_grp_94baec', \n",
    "'n_months_since_lapse_grp_e91421', 'n_months_since_lapse_lh_f852af', 'n_months_since_lapse_lh_947b15']\n",
    "\n",
    "bins = [0, 20, 40, 60, 80, 100, float('inf')]\n",
    "labels = ['0-20', '21-40', '41-60', '61-80', '81-100', '>100']\n",
    "\n",
    "for col in cols:\n",
    "    df[col] = df[col].astype('float64').replace(9999,-1)\n",
    "    null_indices = df[col].index[df[col].isnull()].tolist()\n",
    "    df[col] = pd.cut(df[col], bins=bins, labels=labels, right=False)\n",
    "    df[col] = df[col].cat.add_categories(\"No lapse\")\n",
    "    df[col].loc[null_indices] = \"No lapse\"\n",
    "    df[col] = df[col].cat.add_categories(\"No data\").fillna(\"No data\")\n",
    "    if df[col].isnull().sum() == 0:\n",
    "        print(f\"{col} done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the distribution of columns, for determining the bins when converting to categorical variable\n",
    "\n",
    "print(df['giclaim_amt'].value_counts())\n",
    "\n",
    "plt.hist(df['recency_giclaim'], bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of \"your_column\"')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flg_affconnect_show_interest_ever can change to False (1 change to True)\n",
    "df['flg_affconnect_show_interest_ever'].fillna(False, inplace=True)\n",
    "df['flg_affconnect_show_interest_ever'] = df['flg_affconnect_show_interest_ever'].astype(bool)\n",
    "if df[\"flg_affconnect_show_interest_ever\"].isnull().sum() == 0:\n",
    "    print(\"flg_affconnect_show_interest_ever done\")\n",
    "\n",
    "#flg_affconnect_ready_to_buy_ever can be dropped (very similar to n_months_since_visit_affcon)\n",
    "a = df.shape[1]\n",
    "df.drop(\"flg_affconnect_ready_to_buy_ever\", axis=1, inplace=True)\n",
    "if df.shape[1] + 1 == a:\n",
    "    print(\"Dropped flg_affconnect_ready_to_buy_ever\")\n",
    "print(df.shape)\n",
    "\n",
    "#flg_affconnect_lapse_ever can be dropped (duplicate of have_lapse and flg_affconnect_ready_to_buy_ever)\n",
    "a = df.shape[1]\n",
    "df.drop(\"flg_affconnect_lapse_ever\", axis=1, inplace=True)\n",
    "if df.shape[1] + 1 == a:\n",
    "    print(\"Dropped flg_affconnect_lapse_ever\")\n",
    "print(df.shape)\n",
    "\n",
    "#affcon_visit_days can be changed to 0\n",
    "    #Here we can hypothesise that 16176 people did not buy through affcon\n",
    "df['affcon_visit_days'].fillna(0, inplace=True)\n",
    "if df[\"affcon_visit_days\"].isnull().sum() == 0:\n",
    "    print(\"affcon_visit_days done\")\n",
    "    \n",
    "#n_months_since_visit_affcon can be converted to cat var, replace nan with \"no affcon\".\n",
    "df['n_months_since_visit_affcon'] = df['n_months_since_visit_affcon'].astype('category')\n",
    "df['n_months_since_visit_affcon'] = df['n_months_since_visit_affcon'].cat.add_categories(\"No affcon\").fillna(\"No affcon\")\n",
    "if df[\"n_months_since_visit_affcon\"].isnull().sum() == 0:\n",
    "    print(\"n_months_since_visit_affcon done\")\n",
    "\n",
    "#clmcon_visit_days can be changed to 0\n",
    "    #Here we can hypothesise that 16279 people did not use clmcon\n",
    "df['clmcon_visit_days'].fillna(0, inplace=True)\n",
    "if df[\"clmcon_visit_days\"].isnull().sum() == 0:\n",
    "    print(\"clmcon_visit_days done\")\n",
    "    \n",
    "#recency_clmcon, recency_clmcon_regis can be converted to cat var, replace nan with \"no clmcon\"\n",
    "    #recency_clmcon_regis can use bins of range 20\n",
    "df['recency_clmcon'] = df['recency_clmcon'].astype('category')\n",
    "df['recency_clmcon'] = df['recency_clmcon'].cat.add_categories(\"No clmcon\").fillna(\"No clmcon\")\n",
    "if df[\"recency_clmcon\"].isnull().sum() == 0:\n",
    "    print(\"recency_clmcon done\")\n",
    "\n",
    "bins = [0, 10, 20, 30, 40, 60, 80, float('inf')]\n",
    "labels = ['0-10', '11-20', '21-30', '31-40', '41-60', '61-80', '>80']\n",
    "df['recency_clmcon_regis'] = pd.cut(df['recency_clmcon_regis'], bins=bins, labels=labels, right=False)\n",
    "df['recency_clmcon_regis'] = df['recency_clmcon_regis'].cat.add_categories(\"No clmcon\").fillna(\"No clmcon\")\n",
    "if df[\"recency_clmcon_regis\"].isnull().sum() == 0:\n",
    "    print(\"recency_clmcon_regis done\")\n",
    "    \n",
    "#hlthclaim_amt can be converted to cat var, replace nan with \"no hlthclaim\"\n",
    "    #Low: $0 - $500; Medium Low: $501 - $1,000; Medium: $1,001 - $5,000; High: $5,001 - $10,000; Very High: $10,001 and above\n",
    "    #Here we can hypothesise that 15552 people did not make healthclaims\n",
    "bins = [0, 500, 1000, 5000, 10000, float('inf')]\n",
    "labels = ['Low: $0 - $500', 'Medium Low: $501 - $1,000', 'Medium: $1,001 - $5,000', 'High: $5,001 - $10,000', 'Very High: >$10,000']\n",
    "df['hlthclaim_amt'] = df['hlthclaim_amt'].fillna(-1)\n",
    "df['hlthclaim_amt'] = pd.cut(df['hlthclaim_amt'], bins=bins, labels=labels, right=False)\n",
    "df['hlthclaim_amt'] = df['hlthclaim_amt'].cat.add_categories(\"No healthclaim\").fillna(\"No healthclaim\")\n",
    "if df[\"hlthclaim_amt\"].isnull().sum() == 0:\n",
    "    print(\"hlthclaim_amt done\")\n",
    "    \n",
    "#recency_hlthclaim can be converted to cat var, replace nan with \"no hlthclaim\"\n",
    "    #can use bins of range 20\n",
    "bins = [0, 10, 20, 30, 40, 60, 80, 100, float('inf')]\n",
    "labels = ['0-10', '11-20', '21-30', '31-40', '41-60', '61-80', '81-100', '>100']\n",
    "df['recency_hlthclaim'] = pd.cut(df['recency_hlthclaim'], bins=bins, labels=labels, right=False)\n",
    "df['recency_hlthclaim'] = df['recency_hlthclaim'].cat.add_categories(\"No healthclaim\").fillna(\"No healthclaim\")\n",
    "if df[\"recency_hlthclaim\"].isnull().sum() == 0:\n",
    "    print(\"recency_hlthclaim done\")\n",
    "    \n",
    "#hlthclaim_cnt_success can be converted to cat var, replace nan with \"no success\"\n",
    "    #can use bins of ranges 0-20; 21-40; 41-100; 101-180; >180\n",
    "bins = [0, 20, 40, 100, 180, float('inf')]\n",
    "labels = ['0-20', '21-40', '41-100', '101-180', '>180']\n",
    "df['hlthclaim_cnt_success'] = pd.cut(df['hlthclaim_cnt_success'], bins=bins, labels=labels, right=False)\n",
    "df['hlthclaim_cnt_success'] = df['hlthclaim_cnt_success'].cat.add_categories(\"No success\").fillna(\"No success\")\n",
    "if df[\"hlthclaim_cnt_success\"].isnull().sum() == 0:\n",
    "    print(\"hlthclaim_cnt_success done\")\n",
    "    \n",
    "#recency_hlthclaim_success can be converted to cat var, replace nan with \"no success\"\n",
    "    #can use bins of range 20\n",
    "bins = [0, 10, 20, 30, 40, 60, 80, 100, float('inf')]\n",
    "labels = ['0-10', '11-20', '21-30', '31-40', '41-60', '61-80', '81-100', '>100']\n",
    "df['recency_hlthclaim_success'] = pd.cut(df['recency_hlthclaim_success'], bins=bins, labels=labels, right=False)\n",
    "df['recency_hlthclaim_success'] = df['recency_hlthclaim_success'].cat.add_categories(\"No success\").fillna(\"No success\")\n",
    "if df[\"recency_hlthclaim_success\"].isnull().sum() == 0:\n",
    "    print(\"recency_hlthclaim_success done\")\n",
    "    \n",
    "#hlthclaim_cnt_unsuccess can be converted to cat var, replace nan with \"no unsuccess\"\n",
    "bins = [0, 1, 2, 4, 6, 8, 10, 15, float('inf')]\n",
    "labels = ['1', '2', '3-4', '5-6', '7-8', '9-10', '11-15', '>15']\n",
    "df['hlthclaim_cnt_unsuccess'] = pd.cut(df['hlthclaim_cnt_unsuccess'], bins=bins, labels=labels, right=False)\n",
    "df['hlthclaim_cnt_unsuccess'] = df['hlthclaim_cnt_unsuccess'].cat.add_categories(\"No unsuccess\").fillna(\"No unsuccess\")\n",
    "if df[\"hlthclaim_cnt_unsuccess\"].isnull().sum() == 0:\n",
    "    print(\"hlthclaim_cnt_unsuccess done\")\n",
    "    \n",
    "#recency_hlthclaim_unsuccess can be converted to cat var, replace nan with \"no unsuccess\"\n",
    "    #can use bins of range 20\n",
    "bins = [0, 20, 40, 60, 80, 100, 120, float('inf')]\n",
    "labels = ['0-20', '21-40', '41-60', '61-80', '81-100', '101-120', '>120']\n",
    "df['recency_hlthclaim_unsuccess'] = pd.cut(df['recency_hlthclaim_unsuccess'], bins=bins, labels=labels, right=False)\n",
    "df['recency_hlthclaim_unsuccess'] = df['recency_hlthclaim_unsuccess'].cat.add_categories(\"No unsuccess\").fillna(\"No unsuccess\")\n",
    "if df[\"recency_hlthclaim_unsuccess\"].isnull().sum() == 0:\n",
    "    print(\"recency_hlthclaim_unsuccess done\")\n",
    "\n",
    "#flg_hlthclaim_839f8a_ever can change to False (1 change to True)\n",
    "df['flg_hlthclaim_839f8a_ever'].fillna(False, inplace=True)\n",
    "df['flg_hlthclaim_839f8a_ever'] = df['flg_hlthclaim_839f8a_ever'].astype(bool)\n",
    "if df[\"flg_hlthclaim_839f8a_ever\"].isnull().sum() == 0:\n",
    "    print(\"flg_hlthclaim_839f8a_ever done\")\n",
    "\n",
    "#recency_hlthclaim_839f8a can be converted to cat var, replace nan with \"no claim\"\n",
    "bins = [0, 2, 5, 10, 20, 40, 60, 80, 100, 120, float('inf')]\n",
    "labels = ['0-2', '3-5', '6-10', '11-20', '21-40', '41-60', '61-80', '81-100', '101-120', '>120']\n",
    "df['recency_hlthclaim_839f8a'] = pd.cut(df['recency_hlthclaim_839f8a'], bins=bins, labels=labels, right=False)\n",
    "df['recency_hlthclaim_839f8a'] = df['recency_hlthclaim_839f8a'].cat.add_categories(\"No claim\").fillna(\"No claim\")\n",
    "if df[\"recency_hlthclaim_839f8a\"].isnull().sum() == 0:\n",
    "    print(\"recency_hlthclaim_839f8a done\")\n",
    "    \n",
    "#flg_hlthclaim_14cb37_ever can change to False (1 change to True)\n",
    "df['flg_hlthclaim_14cb37_ever'].fillna(False, inplace=True)\n",
    "df['flg_hlthclaim_14cb37_ever'] = df['flg_hlthclaim_14cb37_ever'].astype(bool)\n",
    "if df[\"flg_hlthclaim_14cb37_ever\"].isnull().sum() == 0:\n",
    "    print(\"flg_hlthclaim_14cb37_ever done\")\n",
    "\n",
    "#recency_hlthclaim_14cb37 can be converted to cat var, replace nan with \"no claim\"\n",
    "bins = [0, 2, 5, 10, 20, 40, 60, 80, 100, 120, float('inf')]\n",
    "labels = ['0-2', '3-5', '6-10', '11-20', '21-40', '41-60', '61-80', '81-100', '101-120', '>120']\n",
    "df['recency_hlthclaim_14cb37'] = pd.cut(df['recency_hlthclaim_14cb37'], bins=bins, labels=labels, right=False)\n",
    "df['recency_hlthclaim_14cb37'] = df['recency_hlthclaim_14cb37'].cat.add_categories(\"No claim\").fillna(\"No claim\")\n",
    "if df[\"recency_hlthclaim_14cb37\"].isnull().sum() == 0:\n",
    "    print(\"recency_hlthclaim_14cb37 done\")\n",
    "    \n",
    "#giclaim_amt can be converted to cat var, replace nan with \"no giclaim\"\n",
    "    #Low: $0 - $500; Medium Low: $501 - $1,000; Medium: $1,001 - $5,000; High: $5,001 - $10,000; Very High: $10,001 and above\n",
    "    #Here we can hypothesise that 16545 people did not make giclaims\n",
    "bins = [0, 500, 1000, 5000, 10000, float('inf')]\n",
    "labels = ['Low: $0 - $500', 'Medium Low: $501 - $1,000', 'Medium: $1,001 - $5,000', 'High: $5,001 - $10,000', 'Very High: >$10,000']\n",
    "df['giclaim_amt'] = df['giclaim_amt'].fillna(-1)\n",
    "df['giclaim_amt'] = pd.cut(df['giclaim_amt'], bins=bins, labels=labels, right=False)\n",
    "df['giclaim_amt'] = df['giclaim_amt'].cat.add_categories(\"No giclaim\").fillna(\"No giclaim\")\n",
    "if df[\"giclaim_amt\"].isnull().sum() == 0:\n",
    "    print(\"giclaim_amt done\")\n",
    "    \n",
    "#recency_giclaim can be converted to cat var, replace nan with \"no giclaim\"\n",
    "    #can use bins of range 10\n",
    "bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, float('inf')]\n",
    "labels = ['0-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100', '>100']\n",
    "df['recency_giclaim'] = pd.cut(df['recency_giclaim'], bins=bins, labels=labels, right=False)\n",
    "df['recency_giclaim'] = df['recency_giclaim'].cat.add_categories(\"No giclaim\").fillna(\"No giclaim\")\n",
    "if df[\"recency_giclaim\"].isnull().sum() == 0:\n",
    "    print(\"recency_giclaim done\")\n",
    "    \n",
    "#f_purchase_lh can change to False (1 change to True)\n",
    "df['f_purchase_lh'].fillna(False, inplace=True)\n",
    "df['f_purchase_lh'] = df['f_purchase_lh'].astype(bool)\n",
    "if df[\"f_purchase_lh\"].isnull().sum() == 0:\n",
    "    print(\"f_purchase_lh done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df.isnull().sum().sum()==0:\n",
    "    print(\"All null values cleared!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert object columns to categorical\n",
    "def convert_to_categorical(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].astype('category')\n",
    "    return df\n",
    "\n",
    "# Convert pandas DataFrame to categorical\n",
    "df = convert_to_categorical(df)\n",
    "\n",
    "# Initialize H2O\n",
    "h2o.init()\n",
    "\n",
    "# Convert pandas DataFrame to H2OFrame\n",
    "h2o_df = h2o.H2OFrame(df)\n",
    "\n",
    "# Split the dataset into a train and valid set:\n",
    "train, valid = h2o_df.split_frame(ratios=[.8], seed=1234)\n",
    "\n",
    "# Training the Model\n",
    "RF_Model = H2ORandomForestEstimator(ntrees=10,\n",
    "                                    max_depth=5,\n",
    "                                    min_rows=10,\n",
    "                                    calibrate_model=True,\n",
    "                                    calibration_frame=valid,\n",
    "                                    binomial_double_trees=True)\n",
    "\n",
    "# Train the Random Forest Model\n",
    "RF_Model.train(x=list(df.columns).remove('f_purchase_lh'),\n",
    "               y='f_purchase_lh',\n",
    "               training_frame=train,\n",
    "               validation_frame=valid)\n",
    "\n",
    "model_path = h2o.save_model(model=RF_Model, path=\"./model\", force=True)\n",
    "print(model_path)\n",
    "\n",
    "# Ensure the input_data is an H2O Frame\n",
    "if not isinstance(df, h2o.H2OFrame):\n",
    "    input_data = h2o.H2OFrame(df)\n",
    "\n",
    "# Eval performance:\n",
    "perf = RF_Model.model_performance()\n",
    "\n",
    "print(\"Performance: \\n\", perf)\n",
    "# Generate predictions on a validation set (if necessary):\n",
    "pred = RF_Model.predict(valid)\n",
    "\n",
    "print(\"Prediction: \\n\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
